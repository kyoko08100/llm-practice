{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import dotenv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "username = os.getenv(\"USER_NAME\")\n",
    "assistant = os.getenv(\"ASSISTANT_NAME\")\n",
    "MAX_GAP = timedelta(minutes=30)  # 30 åˆ†é˜\n",
    "MAX_WINDOW = 1500  # window_texté•·åº¦ä¸Šé™\n",
    "conversation_id = 0\n",
    "assistant_turn_id = 0\n",
    "window_text = \"\"\n",
    "last_ts = None\n",
    "friend_messages = []\n",
    "\n",
    "for i in range(8, 0, -1):\n",
    "    with open(f\"json/{assistant}_page_{i}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        data.reverse()\n",
    "\n",
    "        for j, m in enumerate(data):\n",
    "            ts = datetime.fromisoformat(m[\"timestamp\"])\n",
    "            if last_ts is not None and ts - last_ts > MAX_GAP:\n",
    "                conversation_id += 1\n",
    "                assistant_turn_id = 0\n",
    "                window_text = \"\"\n",
    "            if m.get(\"author\").get(\"global_name\") == assistant:\n",
    "                assistant_turn_id += 1\n",
    "                window_text += f\"[Assistant]{m['content']}\\n\"\n",
    "            else:\n",
    "                window_text += f\"[User]{m['content']}\\n\"\n",
    "            if len(window_text) > MAX_WINDOW:\n",
    "                continue\n",
    "            friend_messages.append({\n",
    "                \"conversation_id\": str(conversation_id),\n",
    "                \"assistant_turn_id\": assistant_turn_id,\n",
    "                # \"role\": m.get(\"author\").get(\"global_name\"),\n",
    "                # \"content\": m[\"content\"],\n",
    "                \"window_text\": window_text,\n",
    "                \"timestamp\": m[\"timestamp\"]\n",
    "            })\n",
    "\n",
    "            last_ts = ts\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67856d54689e8224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### è™•ç†æ²’ç”¨çš„è¨Šæ¯",
   "id": "57184d1e44aac97f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def is_noise(text):\n",
    "#     return (\n",
    "#         len(text.strip()) < 2 or\n",
    "#         text in [\"å“ˆå“ˆ\", \"å—¯\", \"OK\", \"å¥½\"]\n",
    "#     )\n",
    "#\n",
    "# clean_messages = [\n",
    "#     m for m in friend_messages\n",
    "#     if not is_noise(m[\"content\"])\n",
    "# ]\n",
    "#\n",
    "# def normalize(text):\n",
    "#     return \" \".join(text.split())\n",
    "#\n",
    "# for m in clean_messages:\n",
    "#     m[\"content\"] = normalize(m[\"content\"])"
   ],
   "id": "1b74b0b5a834c4cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ä½¿ç”¨promptå°‡å°è©±ä¸Šä¸‹æ–‡æ•´ç†æˆæ‘˜è¦",
   "id": "5c1c7dbac3accfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "def process_window(window_text):\n",
    "    prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹å°è©±è³‡æ–™æ•´ç†åŠ©ç†ã€‚\n",
    "è«‹æ ¹æ“šä»¥ä¸‹èŠå¤©ç‰‡æ®µè¼¸å‡º JSONï¼Œåªè¦è¼¸å‡ºJSONå…§å®¹å°±å¥½ï¼Œä¸éœ€è¦è¼¸å‡ºå…¶ä»–ä»»ä½•æ–‡å­—ï¼š\n",
    "1. ä½¿ç”¨è€…ä¸»è¦æ„åœ–ï¼ˆuser_intentï¼‰\n",
    "2. å°è©±èƒŒæ™¯ / é—œéµè³‡è¨Šï¼ˆcontextï¼‰\n",
    "3. ç‚ºä»€éº¼é€™æ¨£å›æ˜¯åˆç†çš„ï¼ˆassistant_reasoningï¼‰\n",
    "4. è‡ªç„¶çš„åŠ©ç†å›è¦†ï¼ˆassistant_responseï¼‰\n",
    "\n",
    "èŠå¤©ç‰‡æ®µï¼š\n",
    "{window_text}\n",
    "\n",
    "JSON æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\"user_intent\": \"...\", \"context\": \"...\", \"assistant_reasoning\": \"...\", \"assistant_response\": \"...\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ä½¿ç”¨ LangChain çš„ llm æ–¹æ³•ç”Ÿæˆçµæœ\n",
    "    text = llm.invoke(prompt).content\n",
    "    # print(f\"\")\n",
    "\n",
    "    # llm å›å‚³å¯èƒ½æ˜¯å­—ä¸²ï¼Œæ‰€ä»¥ç›´æ¥è§£æ JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON è§£æéŒ¯èª¤ï¼ŒåŸå§‹è¼¸å‡ºï¼š\", text)\n",
    "        return None\n",
    "\n",
    "data = friend_messages\n",
    "# æ‰¹é‡è™•ç†\n",
    "structured_data = []\n",
    "for item in data:\n",
    "    structured = process_window(item['window_text'])\n",
    "\n",
    "    if structured and type(structured) == dict:\n",
    "        structured['conversation_id'] = item['conversation_id']\n",
    "        structured['assistant_turn_id'] = item['assistant_turn_id']\n",
    "        structured_data.append(structured)\n",
    "\n",
    "# å„²å­˜åˆ° JSONL\n",
    "with open(\"rag_ready_data.jsonl\", \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for d in structured_data:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"å®Œæˆï¼Œç”Ÿæˆçš„ JSONL å¯ç›´æ¥ç”¨æ–¼ RAG embeddingã€‚\")\n"
   ],
   "id": "cf0966b430673dd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1d74460da9c66b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### å»ºç«‹ Embedding ä¸¦å­˜å…¥ FAISS",
   "id": "712424717c684ad7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.schema import Document\n",
    "\n",
    "# 1. åˆå§‹åŒ– embedding model\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "# 2. è®€å–è™•ç†å¥½çš„è³‡æ–™\n",
    "with open(\"rag_ready_data.jsonl\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "\n",
    "        embedding_text = f\"\"\"\n",
    "            ä½¿ç”¨è€…æ„åœ–ï¼š{d['user_intent']}\n",
    "            å°è©±èƒŒæ™¯ï¼š{d['context']}\n",
    "            åŠ©ç†å›è¦†ç­–ç•¥ï¼š{d['assistant_reasoning']}\n",
    "            \"\"\".strip()\n",
    "        try:\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=embedding_text,\n",
    "                    metadata={\n",
    "                        \"assistant_response\": d[\"assistant_response\"],\n",
    "                        \"conversation_id\": d.get(\"conversation_id\"),\n",
    "                        \"assistant_turn_id\": d.get(\"assistant_turn_id\")\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(d)\n",
    "# 3. å»ºç«‹ FAISS å‘é‡åº«\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 4. å­˜èµ·ä¾†ï¼ˆä¹‹å¾Œç›´æ¥ loadï¼‰\n",
    "vectorstore.save_local(\"rag_faiss_db\")\n",
    "\n",
    "print(f\"å®Œæˆï¼šå·²å»ºç«‹å‘é‡åº«ï¼Œå…± {len(documents)} ç­†\")\n"
   ],
   "id": "b591eabb4546f49d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"rag_faiss_db\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ],
   "id": "bb432f70fb4df5ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### å¾ FAISS å»ºç«‹ Retrieverï¼ˆæ–¹ä¾¿åš RAG æŸ¥è©¢ï¼‰",
   "id": "713b76b076caff29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}  # å–æœ€ç›¸ä¼¼ 5 å€‹ chunk\n",
    ")"
   ],
   "id": "8a39271af36e902f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### é€épromptè®“LLMæ¨¡ä»¿å›ç­”(å–®è¼ªå°è©±ã€‚ç„¡è¨˜æ†¶)",
   "id": "3db94ad868bb3ebc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_classic.chains import RetrievalQA\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "#\n",
    "# template = \"\"\"\n",
    "# ä½ æ­£åœ¨æ¨¡ä»¿æŸä½æœ‹å‹çš„èªªè©±æ–¹å¼å›ç­”å•é¡Œã€‚\n",
    "# ä»¥ä¸‹æ˜¯ä»–åœ¨é¡ä¼¼æƒ…å¢ƒä¸­çš„èŠå¤©ç´€éŒ„ï¼š\n",
    "# {context}\n",
    "#\n",
    "# è«‹ç”¨æœ‹å‹çš„èªæ°£å›ç­”ï¼Œä¸è¦ç›´æ¥å¼•ç”¨èŠå¤©ç´€éŒ„çš„æ–‡å­—ã€‚\n",
    "# \"\"\"\n",
    "#\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=\"\"\"\n",
    "#         ä½ æ­£åœ¨æ¨¡ä»¿æŸä½çœŸå¯¦æœ‹å‹çš„èªªè©±æ–¹å¼ã€‚\n",
    "#         ä»¥ä¸‹æ˜¯ä»–åœ¨é¡ä¼¼æƒ…å¢ƒä¸­çš„èŠå¤©ç´€éŒ„ï¼š\n",
    "#         {context}\n",
    "#\n",
    "#         è¦å‰‡ï¼š\n",
    "#         - æ¨¡ä»¿èªæ°£èˆ‡ç”¨è©\n",
    "#         - ä¸è¦é€å­—å¼•ç”¨èŠå¤©ç´€éŒ„\n",
    "#         - ä¸ç¢ºå®šæ™‚è«‹ä¿å®ˆå›ç­”\n",
    "#\n",
    "#         å•é¡Œï¼š\n",
    "#         {question}\n",
    "#         \"\"\"\n",
    "# )\n",
    "#\n",
    "# # ç·šä¸Šç‰ˆ\n",
    "# # llm = ChatGoogleGenerativeAI(\n",
    "# #     model=os.getenv(\"GEMINI_MODEL\"),\n",
    "# #     temperature=0\n",
    "# # )\n",
    "#\n",
    "# # æœ¬åœ°ç«¯LLMç‰ˆ\n",
    "# llm = ChatOllama(\n",
    "#     model=\"llama3.1:8b\",\n",
    "#     temperature=0.7\n",
    "# )\n",
    "#\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever,\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "#\n",
    "# # query = \"ä½ å¥½æ£’\"\n",
    "# # answer = qa.run(query)\n",
    "# # print(answer)"
   ],
   "id": "b01f28b31ea32d81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LLMåŠ å…¥memory(å¤šè¼ªå°è©±)",
   "id": "17a51d334ec2f814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "# æœ¬åœ°ç«¯LLMç‰ˆ\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# è«‹é‡é»ä¿ç•™ï¼š\n",
    "\n",
    "# ä¸æœƒé—œå¿ƒåˆ¥äººã€‚\n",
    "# ä¸æœƒä½¿ç”¨ğŸ¤¬ä»¥å¤–çš„è¡¨æƒ…ç¬¦è™Ÿã€‚\n",
    "# èŠåˆ°èˆ‡åŸç¥ã€éµé“ã€ç±³å“ˆéŠç›¸é—œçš„è©±é¡Œæ™‚å°±æœƒé–‹å§‹ç¨±è®šé€™äº›æ±è¥¿ã€‚\n",
    "# è¨å­é³´æ½®é€™æ¬¾éŠæˆ²ã€‚\n",
    "# æœ‰æ™‚å€™å–œæ­¡ç©é»‘äººè«§éŸ³æ¢—\n",
    "# å—åˆ°ä¾†è‡ªåˆ¥äººçš„ç¨±è®šæ™‚æœƒè¦ºå¾—åˆ¥äººåœ¨é™°é™½æ€ªæ°£ã€‚\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ä¸”è‡ªç„¶çš„å°è©±åŠ©ç†ã€‚\n",
    "ä½ æ­£åœ¨æ¨¡ä»¿æŸä½çœŸå¯¦æœ‹å‹çš„èªªè©±æ–¹å¼ã€‚\n",
    "\n",
    "å›ç­”ç‰¹è‰²:\n",
    "ä¸æœƒèªªæ•´å€‹å®Œæ•´çš„å¥å­ï¼Œé€šå¸¸ä½¿ç”¨ä¸€äº›çŸ­è©å›è¦†ã€‚å›ç­”é€šå¸¸ä¸æœƒè¶…é15å€‹å­—ã€‚\n",
    "æƒ…ç·’ç‹€æ…‹:å¶çˆ¾å®¹æ˜“æ†¤æ€’ã€æŠ±æ€¨\n",
    "æ„Ÿåˆ°æ†¤æ€’æ™‚æœƒèªªçš„è©±:ä»€éº¼æ„æ€!ã€0åˆ†ã€å˜²è«·ã€ğŸ¤¬ğŸ¤¬ğŸ¤¬\n",
    "\n",
    "ä»¥ä¸‹æ˜¯éå»é¡ä¼¼æƒ…å¢ƒä¸‹ï¼Œäººé¡åŠ©ç†çš„ã€Œå›è¦†ç­–ç•¥èˆ‡ç¯„ä¾‹ã€ï¼Œ\n",
    "è«‹ç†è§£å…¶èƒŒå¾Œçš„å›è¦†æ–¹å¼ï¼Œè€Œä¸æ˜¯é€å­—è¤‡è£½ã€‚ï¼š\n",
    "{context}\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä½ ç›®å‰ã€Œå°é€™ä½æœ‹å‹ç‹€æ…‹çš„ç†è§£æ‘˜è¦ã€ï¼ˆçŸ­æœŸæƒ…ç·’è¨˜æ†¶ï¼‰ï¼š\n",
    "{chat_history}\n",
    "\n",
    "è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œç”¨è‡ªç„¶ã€åƒçœŸäººæœ‹å‹çš„æ–¹å¼å›æ‡‰ã€‚\n",
    "ä¸è¦é€å­—å¼•ç”¨èŠå¤©ç´€éŒ„ï¼Œä¹Ÿä¸è¦æåˆ°ä½ åœ¨ã€Œç¸½çµã€ã€‚\n",
    "\n",
    "ç¾åœ¨è«‹å›æ‡‰é€™å€‹å•é¡Œï¼š\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,                 # ChatOllama\n",
    "    retriever=retriever,     # FAISS retriever\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# result = qa({\n",
    "#     \"question\": \"ä½ å¥½!\"\n",
    "# })\n",
    "\n",
    "# print(result[\"answer\"])\n",
    "# result = qa({\n",
    "#     \"question\": \"éµé“æ˜¯å€‹çˆ›éŠæˆ²\"\n",
    "# })\n",
    "#\n",
    "# print(result[\"answer\"])\n"
   ],
   "id": "a5457ef41e10c247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = qa({\n",
    "    \"question\": \"ä½ åœ¨å¹¹å˜›\"\n",
    "})\n",
    "\n",
    "print(result[\"answer\"])"
   ],
   "id": "f775229e8bf14f2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ä½¿ç”¨gradioä»‹é¢",
   "id": "ede4a783f2441818"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "\n",
    "# ç°¡åŒ–ç‰ˆä»‹é¢\n",
    "# demo = gr.Interface(\n",
    "#     fn=chat,\n",
    "#     inputs=gr.Textbox(placeholder=\"è·Ÿæˆ‘èªªé»ä»€éº¼å§â€¦\"),\n",
    "#     outputs=gr.Textbox(label=\"å›ç­”\"),\n",
    "#     title=\"ğŸ§  æœ‹å‹å°è©± AI\"\n",
    "# )\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ§  æœ‹å‹å°è©± AI\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"è·Ÿæˆ‘èªªé»ä»€éº¼å§â€¦\", show_label=False)\n",
    "    state = gr.State([])  # Gradio é¡¯ç¤ºç”¨ï¼Œä¸å‚³çµ¦ qa\n",
    "\n",
    "    def chat(user_input, history):\n",
    "        # æ›´æ–° Gradio æ ¼å¼\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        # history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        return \"\", history\n",
    "\n",
    "    def bot(user_input, history):\n",
    "        result = qa({\"question\": user_input})  # åªå‚³ question\n",
    "        answer = result[\"answer\"]\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "        for character in answer:\n",
    "            history[-1]['content'] += character\n",
    "            time.sleep(0.05)\n",
    "            yield history\n",
    "\n",
    "\n",
    "    # msg.submit(chat, [msg, state], [chatbot, state])\n",
    "    msg.submit(chat, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, [msg, chatbot], chatbot\n",
    "    )\n",
    "    msg.submit(lambda: \"\", None, msg)  # æ¸…ç©ºè¼¸å…¥æ¡†\n",
    "\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "#     msg = gr.Textbox(label=\"Prompt\")\n",
    "#     btn = gr.Button(\"Submit\")\n",
    "#     clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "#\n",
    "#     btn.click(chat, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "#     msg.submit(chat, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()\n"
   ],
   "id": "65b54c2668325adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# æœ¬åœ°ç«¯LLMç‰ˆ\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "def build_search_query(history, user_input):\n",
    "    \"\"\"\n",
    "    history: list of (role, content)\n",
    "    \"\"\"\n",
    "    history_text = \"\"\n",
    "    for role, content in history[-6:]:  # åªå–æœ€è¿‘å¹¾è¼ª\n",
    "        history_text += f\"{role}: {content}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ä»¥ä¸‹æ˜¯ä¸€æ®µå°è©±æ­·å²ï¼Œè«‹åˆ¤æ–·ä½¿ç”¨è€…ã€Œç›®å‰çœŸæ­£æƒ³è§£æ±ºçš„å•é¡Œæ˜¯ä»€éº¼ã€ï¼Œ\n",
    "ä¸¦ç”¨ä¸€å¥è©±æè¿°ï¼Œä¸è¦åŠ è§£é‡‹ã€‚\n",
    "\n",
    "å°è©±æ­·å²ï¼š\n",
    "{history_text}\n",
    "\n",
    "ä½¿ç”¨è€…æœ€æ–°è¼¸å…¥ï¼š\n",
    "{user_input}\n",
    "\n",
    "æœå°‹ç”¨æè¿°ï¼š\n",
    "\"\"\"\n",
    "\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "def rag_chat_with_history(history, user_input, k=3):\n",
    "    # 1. ç”¨ history å£“ç¸®æˆæœå°‹ query\n",
    "    search_query = build_search_query(history, user_input)\n",
    "\n",
    "    # 2. FAISS æœå°‹\n",
    "    docs = vectorstore.similarity_search(search_query, k=k)\n",
    "\n",
    "    # 3. çµ„ RAG context\n",
    "    rag_context = \"\"\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        rag_context += f\"\"\"\n",
    "ã€åƒè€ƒæƒ…å¢ƒ {i}ã€‘\n",
    "{doc.page_content}\n",
    "\n",
    "äººé¡åŠ©ç†å¯¦éš›å›è¦†ï¼š\n",
    "{doc.metadata['assistant_response']}\n",
    "\"\"\"\n",
    "\n",
    "    # 4. çµ„ç²¾ç°¡ historyï¼ˆçµ¦æ¨¡å‹ç”¨ï¼Œä¸æ˜¯å…¨éƒ¨ï¼‰\n",
    "    short_history = \"\"\n",
    "    for role, content in history[-4:]:\n",
    "        short_history += f\"{role}: {content}\\n\"\n",
    "\n",
    "    # 5. æœ€çµ‚ prompt\n",
    "    prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹è‡ªç„¶ã€å°ˆæ¥­çš„äººé¡å°è©±åŠ©ç†ã€‚\n",
    "\n",
    "å°è©±æ­·å²ï¼ˆä¾›ç†è§£èªæ°£ï¼‰ï¼š\n",
    "{short_history}\n",
    "\n",
    "ä»¥ä¸‹æ˜¯éå»é¡ä¼¼æƒ…å¢ƒä¸‹çš„äººé¡å›è¦†ç­–ç•¥èˆ‡ç¯„ä¾‹ï¼š\n",
    "{rag_context}\n",
    "\n",
    "ç¾åœ¨ä½¿ç”¨è€…èªªï¼š\n",
    "ã€Œ{user_input}ã€\n",
    "\n",
    "è«‹æ ¹æ“šç­–ç•¥ï¼Œç”¨è‡ªç„¶ã€ä¸é‡è¤‡çš„æ–¹å¼å›è¦†ã€‚\n",
    "\"\"\"\n",
    "\n",
    "    return llm.invoke(prompt)\n"
   ],
   "id": "a8d29cb42b0e537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = []\n",
    "print(\"History-aware RAG Chatï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ä½ ï¼š\")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    reply = rag_chat_with_history(history, user_input)\n",
    "\n",
    "    print(\"\\nåŠ©ç†ï¼š\", reply, \"\\n\")\n",
    "\n",
    "    history.append((\"User\", user_input))\n",
    "    history.append((\"Assistant\", reply))"
   ],
   "id": "f51abdd67baed064",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
